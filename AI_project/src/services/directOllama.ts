import axios from 'axios'

interface Message {
  role: 'user' | 'assistant' | 'system'
  content: string
}

// Available models
export const availableModels = {
  'deepseek-coder:1.3b': { name: 'DeepSeek Coder 1.3b', description: '‚ö° Fast & Lightweight (776MB)', icon: 'üöÄ' },
  'mistral:latest': { name: 'Mistral Latest', description: 'üß† Powerful but Slower (4.4GB)', icon: 'ü§ñ' },
  'llama3.2:latest': { name: 'Llama 3.2', description: 'ü¶ô Balanced Performance (2GB)', icon: 'ü¶ô' }
} as const

export type ModelKey = keyof typeof availableModels

// Direct connection to Ollama bypassing Open WebUI entirely
export const sendDirectOllamaMessage = async (messages: Message[], model: ModelKey = 'deepseek-coder:1.3b'): Promise<string> => {
  try {
    console.log('ü§ñ Connecting directly to Ollama (bypassing Open WebUI)...')
    console.log('üì§ Sending messages:', messages)
    
    const requestBody = {
      model: model,
      messages: messages,
      stream: false,
      options: {
        temperature: 0.7,
        num_predict: model.includes('deepseek') ? 2048 : 4096 // Smaller context for faster models
      }
    }
    
    console.log(`üéØ Using model: ${availableModels[model].name} (${model})`)
    
    console.log('üìã Request body:', JSON.stringify(requestBody, null, 2))
    
    // Try direct connection first, then proxy fallback
    let response
    try {
      console.log('üîó Attempting direct connection to Ollama...')
      response = await axios.post('http://localhost:11434/api/chat', requestBody, {
        timeout: 120000, // 2 minute timeout for complex questions
        headers: {
          'Content-Type': 'application/json'
        },
        withCredentials: false,
      })
    } catch (directError) {
      console.log('üîÑ Direct connection failed, trying proxy...', directError)
      // Fallback to Vite dev server proxy
      response = await axios.post('/direct-ollama/api/chat', requestBody, {
        timeout: 120000,
        headers: {
          'Content-Type': 'application/json'
        },
        withCredentials: false,
      })
    }

    console.log('üì• Ollama response:', response.data)
    
    if (response.data?.message?.content) {
      console.log('‚úÖ Successfully extracted response content')
      return response.data.message.content
    } else {
      console.error('‚ùå Invalid response format:', response.data)
      throw new Error(`Invalid response format from Ollama. Got: ${JSON.stringify(response.data)}`)
    }
  } catch (error) {
    console.error('Direct Ollama connection error:', error)
    
    if (axios.isAxiosError(error)) {
      if (error.code === 'ECONNREFUSED') {
        throw new Error('‚ùå Cannot connect to Ollama. Please start Ollama:\n\n1. Run: ollama serve\n2. Make sure Mistral is installed: ollama pull mistral')
      } else if (error.code === 'ECONNRESET' || error.code === 'ETIMEDOUT') {
        throw new Error('‚è±Ô∏è Connection timeout. Ollama might be processing a complex request. Please try again.')
      } else if (error.response?.status === 404) {
        throw new Error('üîç Model not found. Please install Mistral: ollama pull mistral')
      } else {
        throw new Error(`üö® Ollama error: ${error.message}`)
      }
    }
    
    throw new Error('üîß Failed to connect to Ollama. Please check that Ollama is running.')
  }
}

// Test connection to Ollama
export const testOllamaConnection = async (): Promise<boolean> => {
  try {
    const response = await axios.get('http://localhost:11434/api/version', {
      timeout: 10000, // Increased timeout to 10 seconds
      headers: {
        'Accept': 'application/json'
      }
    })
    console.log('Ollama connection test successful:', response.data)
    return response.status === 200
  } catch (error) {
    console.error('Ollama connection test failed:', error)
    // Even if test fails, Ollama might still be working
    // Let's try a simple version check
    try {
      const fallbackResponse = await fetch('http://localhost:11434/api/version')
      return fallbackResponse.ok
    } catch (fallbackError) {
      console.error('Fallback test also failed:', fallbackError)
      return false
    }
  }
}

// Get available models from Ollama
export const getOllamaModels = async (): Promise<string[]> => {
  try {
    const response = await axios.get('http://localhost:11434/api/tags', {
      timeout: 10000
    })
    
    if (response.data?.models) {
      return response.data.models.map((model: any) => model.name)
    }
    return []
  } catch (error) {
    console.error('Failed to get Ollama models:', error)
    return []
  }
}